{
  "cells": [
    {
      "metadata": {
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "collapsed": true,
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "trusted": false
      },
      "cell_type": "markdown",
      "source": "# LSTM_2\n- 逐次word embedding\n- Google Newsの奴だと簡単にembeddingできた\n> http://kento1109.hatenablog.com/entry/2018/03/15/153652"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "92bd3980eeb6c98e497e203d24e9eb9bdf1c8e0a"
      },
      "cell_type": "code",
      "source": "import os\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom tqdm import tqdm\nimport time\nimport sys\nimport re\n\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.autograd as autograd\nimport torch.nn.functional as F\nimport torch.utils.data\nfrom sklearn.metrics import f1_score\n\nfrom sklearn.model_selection import train_test_split\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n#device = torch.device(\"cpu\")\ndata_dir = '../input/'\n\nsubmission = pd.read_csv(data_dir+'sample_submission.csv',index_col=[0])\n\n# 最大系列長\nmax_length = 40",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "889d7d378d4713d28a156a9cf6a1bb7067732d8d"
      },
      "cell_type": "code",
      "source": "raw_train = pd.read_csv(data_dir+\"train.csv\")\nraw_test = pd.read_csv(data_dir+\"test.csv\")\nraw_train['sen_len'] = raw_train['question_text'].apply(lambda x: len(x.split()) if(len(x.split())<=max_length)\\\n                                                        else max_length)\nraw_test['sen_len'] = raw_test['question_text'].apply(lambda x: len(x.split()) if(len(x.split())<=max_length)\\\n                                                        else max_length)\ntest_X = raw_test['question_text']\ndel raw_test\n\n# train, validデータの分割\ndef train_valid_extract(raw_train,n_train1,n_train0,n_valid0):\n    df_1 = raw_train[raw_train['target']==1]\n    df_0 = raw_train[raw_train['target']==0]\n\n    train_1 = df_1.sample(n=n_train1, random_state=0)\n    train_0 = df_0.sample(n=n_train0, random_state=0)\n    train_df = pd.concat([train_1,train_0])\n\n    valid_1 = df_1[~df_1.index.isin(train_1.index)]\n    valid_0 = df_0[~df_0.index.isin(train_0.index)].sample(n=n_valid0, random_state=0)\n    valid_df = pd.concat([valid_1,valid_0])\n\n    train1_X = train_1['question_text']\n    train0_X = train_0['question_text']\n    valid_X = valid_df['question_text']\n    valid_y = valid_df['target']\n    \n    return train1_X,train0_X,valid_X,valid_y",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "f7ea83b3e1eabf9ad5c77bba11a5e06b3ea65219"
      },
      "cell_type": "code",
      "source": "# data sampling paramater\nn_train1 = 75000\nn_train0 = 1000000\nn_valid0 = 60000 # n_valid1が約5~10%になるように調整\ntrain1_X,train0_X,valid_X,valid_y = train_valid_extract(raw_train,n_train1,n_train0,n_valid0)\ndel raw_train",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "43f179418cd5990bc3d317e434b91fa71dee3bd2"
      },
      "cell_type": "code",
      "source": "# embeddingのための辞書生成\ndef word2id_vec(f_path,id2vec=True):\n    f = open(data_dir+f_path,encoding=\"utf-8\")\n    word2id = {'<PAD>':0,'<UNK>':1} # 単語:id\n    id2vec_list = [np.zeros(300).reshape(1,300),-1*np.ones(300).reshape(1,300)]\n    for i,line in tqdm(enumerate(f)):\n        values = line.split(\" \")\n        word = values[0]\n        word2id[word] = i+2\n        if id2vec == True:\n            weight = np.asarray(values[1:], dtype='float32').reshape(1,300)\n            id2vec_list.append(weight)\n    f.close()\n    id2vec = torch.FloatTensor(np.concatenate(id2vec_list, axis=0))\n    print(len(word2id),len(id2vec))\n    return word2id, id2vec",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "c65273ea2acc5af4da5d82d230036bede24dd2a6"
      },
      "cell_type": "code",
      "source": "f_path = 'embeddings/glove.840B.300d/glove.840B.300d.txt'\nword2id, id2vec = word2id_vec(f_path)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "b831d439499e44d94a6c21d6011cd173d66cc739"
      },
      "cell_type": "code",
      "source": "# Convert values to embeddings\ndef sentence_to_id(text,max_length):\n    #text = text[:-1].split()[:max_length]\n    text = np.array(re.split(\"[ ,.:;!?-]\",text)[:max_length])\n    text = text[~(text == '')]\n    embeds = [word2id.get(x) for x in text]\n    embeds+= [0] * (max_length - len(embeds))\n    #unknown wordはNoneで返されるため、1で置換\n    embeds = np.array(embeds)\n    embeds[embeds == None] = 1\n    return embeds.astype(np.int64)\n\n# data to dataloader\ndef make_dataloader(X,y=None,batch_size=2000,len_X=None,shuffle=True):\n    '''\n    X: torch.tensor \n    y: numpy.array\n    len_X: numpy.array\n    '''\n    if y is not None: # testデータではないとき\n        tensor_y = torch.from_numpy(np.array(y))\n    else : # testデータ、または全ラベルが同じデータセットの時\n        tensor_y = torch.from_numpy(np.zeros(len(X))) #testデータに疑似ラベルを作成\n        \n    if len_X is not None: #文長の情報も用いる場合\n        tensor_len_X = torch.from_numpy(np.array(len_X))\n        data = torch.utils.data.TensorDataset(X,tensor_y,tensor_len_X)\n    else : \n        data = torch.utils.data.TensorDataset(X,tensor_y) # create your datset \n    data_loader =  torch.utils.data.DataLoader(data,batch_size=batch_size, shuffle=shuffle) # create your dataloader \n    \n    return data_loader",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "8f003564e8faacbdd7e9257be49e81c4321246c7",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# word embedding\ntrain1_ids = torch.stack([torch.LongTensor(sentence_to_id(X_text,max_length)) for X_text in tqdm(train1_X)])\ntrain0_ids = torch.stack([torch.LongTensor(sentence_to_id(X_text,max_length)) for X_text in tqdm(train0_X)])\nvalid_ids = torch.stack([torch.LongTensor(sentence_to_id(X_text,max_length)) for X_text in tqdm(valid_X)])\ntest_ids = torch.stack([torch.LongTensor(sentence_to_id(X_text,max_length)) for X_text in tqdm(test_X)])\n\ndel train1_X,train0_X,valid_X,test_X,word2id",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "e17fc8b49165e2318c28590097eedb18cdd6c732"
      },
      "cell_type": "code",
      "source": "batch_size = 3750\n#batch1_ratio = 0.3\nbatch1_size = 1250 #int(batch_size*batch1_ratio)\nbatch0_size = 2500 #int(batch_size*(1-batch1_ratio))\n\nrepeat_num =  n_train1//batch1_size # クラス1の重複回数\n\ntrain1_loader = make_dataloader(train1_ids.repeat(repeat_num,1),\n                                np.ones(len(train1_ids.repeat(repeat_num,1))),\n                                batch_size=batch1_size,len_X=None,shuffle=True)\ntrain0_loader = make_dataloader(train0_ids,np.zeros(len(train0_ids)),batch_size=batch0_size,len_X=None,shuffle=True)\nvalid_loader = make_dataloader(valid_ids,valid_y,len_X=None,shuffle=True)\ntest_loader = make_dataloader(test_ids,len_X=None,shuffle=False)\ndel train1_ids,train0_ids,valid_ids,test_ids",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "0874164659f3438d46072f8ae8d914c69e5b7941"
      },
      "cell_type": "markdown",
      "source": "# モデル作成"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "4354bc09b3f544f98e82086e7126ddcb4b09fbc7"
      },
      "cell_type": "code",
      "source": "class LSTM(nn.Module):\n    def __init__(self,vocab_size, embed_size, hidden_size,output_size,bidirectional,\n                 num_layers=2,dropout=0):\n        \"\"\"\n        :param input_size: int, 入力言語の語彙数\n        :param hidden_size: int, 隠れ層のユニット数\n        \"\"\"\n        super(LSTM, self).__init__()\n        self.hidden_size = hidden_size\n        self.output_size = output_size\n        self.num_layers = num_layers\n        self.dropout = dropout\n        \n        self.embedding = nn.Embedding.from_pretrained(id2vec)\n        self.embedding.weight.requires_grad = False\n        '''\n        self.gru = nn.GRU(input_size=embed_size, hidden_size=self.hidden_size,\n                          num_layers=self.num_layers,dropout=self.dropout,\n                          batch_first=True)\n        '''\n        self.gru = nn.GRU(input_size=embed_size, hidden_size=self.hidden_size,\n                          num_layers=1,dropout=0,\n                          bidirectional=bidirectional,batch_first=True)\n        self.dropout = nn.Dropout()\n        self.fc = nn.Linear(self.hidden_size, self.hidden_size//2)\n        self.fc2 = nn.Linear(self.hidden_size//2, self.output_size)\n        \n    def forward(self, seqs, hidden=None):\n        \"\"\"\n        :param seqs: tensor, 入力のバッチ, size=(max_length, batch_size)\n        :param input_lengths: 入力のバッチの各サンプルの文長\n        :param hidden: tensor, 隠れ状態の初期値, Noneの場合は0で初期化される\n        :return output: tensor, Encoderの出力, size=(max_length, batch_size, hidden_size)\n        :return hidden: tensor, Encoderの隠れ状態, size=(1, batch_size, hidden_size)\n        \"\"\"\n        emb = self.embedding(seqs)\n        output, hidden = self.gru(emb, hidden)\n        x = F.relu(self.fc(output[:,-1,:])) # 系列の最後の出力のみ取り出す。クラス数分の次元のはず\n        x = self.dropout(x)\n        return self.fc2(x)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "86bb617d060e8f726ec13899d2aa8d6cff283816"
      },
      "cell_type": "code",
      "source": "def multi_weighted_log_loss(input, target, class_weight=None):\n    '''\n    class_weight: np.array([a1,a2,...])\n    '''\n    if target.device.type == 'cuda':\n        input = input.to(torch.device(\"cpu\"))\n        target = target.to(torch.device(\"cpu\"))\n    #真分類値tのonehot化\n    batch_size = len(target)\n\n    t_onehot = torch.FloatTensor(batch_size, num_classes)\n    t_onehot.zero_()\n    t_onehot = t_onehot.scatter_(1,target.reshape(batch_size,1),1)\n    if class_weight is None:\n        w = torch.ones(num_classes).type(torch.FloatTensor)\n    else: w = torch.from_numpy(class_weight).type(torch.FloatTensor)\n\n    t_y = (torch.log(input)*t_onehot) # y_ij * ln(p_ij) \n    sum_t = -1 * (torch.sum(t_y,dim=0)/torch.bincount(target).type(torch.FloatTensor)) # -1/N_i sum( t_y )\n    return torch.dot(sum_t,w)/w.sum()\n\ndef compute_loss(batch_X, batch_Y, model, criterion,\n                 class_weight=None,\n                 optimizer=None, is_train=True):\n    # バッチの損失を計算\n    model.train(is_train)\n    \n    if is_train:\n        optimizer.zero_grad()\n    \n    y = model(batch_X)\n    y = F.softmax(y, dim=-1)\n    if class_weight is not None: #クラス重みづけができる損失関数の場合\n        loss = criterion(y, batch_Y,class_weight=class_weight)\n    else: #それ以外の損失関数\n        loss = criterion(y, batch_Y)\n    \n    if is_train:\n        loss.backward()\n        optimizer.step()\n    pred = y.argmax(dim=1).tolist()\n    return loss.item(), batch_Y.data, pred",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "013130054e0691800e18a9ce6c2295182fd55e53"
      },
      "cell_type": "code",
      "source": "model_args = {\n    'vocab_size':len(id2vec),\n    'embed_size':300,\n    'hidden_size':300,\n    'output_size':2,\n    'num_layers':2,\n    'dropout':0.25,\n    'bidirectional':False\n}\nnum_classes=2\nclass_weight = np.array([10,1])\nlr = 0.005\nnum_epochs = 2\nckpt_path = 'lstm_online_embedding.pth'\n\n# model\nmodel = LSTM(**model_args).to(device)\n\noptimizer = optim.Adam(model.parameters(), lr=lr)\ncriterion = multi_weighted_log_loss\n\nscore = f1_score\nloss_epoch_train_ls = []\nloss_epoch_valid_ls = []\nscore_epoch_train_ls = []\nscore_epoch_valid_ls = []\nbest_valid_score = 0",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "e81d2fe6dc68aafd7c86fbedd4173ef4e9ba794c",
        "scrolled": false
      },
      "cell_type": "code",
      "source": "start = time.time()\nfor epoch in range(1, num_epochs+1):\n    train_losses = []\n    valid_losses = []\n    preds_train = []\n    preds_valid = []\n    trues_train = []\n    trues_valid = []\n    # train\n    for i, ((batch1_X,batch1_y), (batch0_X,batch0_y)) in enumerate(zip(train1_loader,train0_loader)):\n        batch_X = torch.cat([batch1_X, batch0_X], dim=0).numpy()\n        batch_Y = torch.cat([batch1_y, batch0_y], dim=0).numpy()\n        # クラス0とクラス1のデータをランダムに混ぜる\n        np.random.seed(i)\n        batch_X = torch.LongTensor(np.random.permutation(batch_X)).to(device)\n        np.random.seed(i)\n        batch_Y = torch.LongTensor(np.random.permutation(batch_Y)).to(device)\n        # 学習\n        train_loss, true,pred = compute_loss(\n            batch_X, batch_Y, model ,criterion, class_weight ,optimizer, is_train=True\n            )\n        train_losses.append(train_loss)\n        trues_train.extend(true)\n        preds_train.extend(pred)\n        #if i % 50 == 0: print(i)\n    # valid\n    for batch in valid_loader:\n        batch_X, batch_Y = batch\n        batch_X = batch_X.to(device)\n        batch_Y = batch_Y.to(device)\n        valid_loss, true,pred = compute_loss(\n            batch_X, batch_Y, model, criterion, class_weight ,is_train=False\n            )\n        valid_losses.append(valid_loss)\n        trues_valid.extend(true)\n        preds_valid.extend(pred)\n    \n    loss_epoch_train = np.mean(train_losses)\n    loss_epoch_valid = np.mean(valid_losses)\n    loss_epoch_train_ls.append(loss_epoch_train)\n    loss_epoch_valid_ls.append(loss_epoch_valid)\n    \n    train_score = score(np.array(trues_train), np.array(preds_train))\n    valid_score = score(np.array(trues_valid), np.array(preds_valid))\n    score_epoch_train_ls.append(train_score)\n    score_epoch_valid_ls.append(valid_score)\n    \n    if valid_score > best_valid_score:\n        ckpt = model.state_dict()\n        torch.save(ckpt, ckpt_path)\n        best_valid_score = valid_score\n\n    print('Time:{:.1f}, Epoch:{}, train_loss: {:.3f}  train_score: {:.3f}  valid_loss: {:.3f}  valid_score: {:.3f}'.format(\n            time.time()-start,epoch, \n            loss_epoch_train, \n            train_score,\n            loss_epoch_valid,\n            valid_score\n    ))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "097ad0993f5b8c93ede45937f89e9d964dc34df8"
      },
      "cell_type": "markdown",
      "source": "# 予測"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "5b1a039816fedf175ad919a5ba3bd817b165a08b"
      },
      "cell_type": "code",
      "source": "# testデータセットに適用\n#del train_loader,valid_loader\npreds_test = []\nfor batch_X,batch_Y in test_loader:\n    batch_X = batch_X.to(device)\n    batch_Y = batch_Y.type(torch.LongTensor).to(device)\n    test_loss, true,pred = compute_loss(\n        batch_X, batch_Y, model, criterion, is_train=False) \n    \n    # モデルの出力を予測値のスカラーに変換\n    preds_test.extend(pred)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "9a4dc97e97899e5b1782c7f4a0008e4f4f35f7b5"
      },
      "cell_type": "code",
      "source": "submission['prediction'] = preds_test\nsubmission.to_csv('submission.csv')\nsubmission.sum()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "ded43e7faadefc0eaaa64f0f3159f74e60795205"
      },
      "cell_type": "code",
      "source": "df_loss_score = pd.DataFrame({'train_loss': loss_epoch_train_ls,\n                           'valid_loss':loss_epoch_valid_ls,\n                           'train_score': score_epoch_train_ls,\n                           'valid_score':score_epoch_valid_ls\n                          })\ndf_loss_score",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}